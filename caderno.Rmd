---
title: "Correlation Coefficient"
output:
  html_document:
    df_print: paged
---

$\rho = \frac{1}{n}\sum\limits_{i=1}^{n}(\frac{x_i-\mu_x}{\sigma_x})*(\frac{y_i-\mu_y}{\sigma_y})$ 

```{r}

library(HistData)
library(tidyverse)
data("GaltonFamilies")
galton_heights <- GaltonFamilies%>%
  filter(childNum == 1 & gender == "male")%>%
  select(father, childHeight) %>%
  rename(son = childHeight)
galton_heights %>% summarise(cor(father, son))

```

Sample correlation is a random variable. And is the best predictor for the population correlation,

```{r}
set.seed(0)
R <- sample_n(galton_heights, 25, replace = T) %>%
  summarise(cor(father, son))
R
B <- 1000
R <- replicate(B, {
  sample_n(galton_heights, 25, replace = T) %>%
  summarise(r = cor(father, son))%>%
    .$r
})
data.frame(R)%>%
  ggplot(aes(R))+geom_histogram(binwidth = 0.05, color = "black")
```

The correlation can also be applied with the Central Limit Theorem, therefore the correlation coefficient can be model by
a normal variable with parameters:

$R \sim N(\rho,\sqrt(\frac{1-r^2}{N-2})$

Correlation is not always a good solution to infer trend between variables
 - One example is the Ascombe quartet, a conjunct of artificial data with different trends, in which all of them produce the same correlation coefficient
 
##Stratification

One way to predict son height based on the sample information and the father's height is to do a stratification of the population and compute the conditional average:

```{r}
conditional_average <- galton_heights %>%
  filter(round(father)==72)%>%
  summarise(avg = mean(son))%>%
  .$avg
conditional_average
```
This is larger than the average son.

stratification by boxplot can help to identify these stratas:

```{r}
galton_heights%>%
  mutate(father_strata = factor(round(father)))%>%
  ggplot(aes(x = father_strata,y = son))+
  geom_boxplot()+
  geom_point()
```

The means of each group seem to have a linear relationship:
```{r}
galton_heights%>%
  mutate(father_strata = factor(round(father)))%>%
  select(father_strata, son)%>%
  group_by(father_strata)%>%
  summarise(mean = mean(son))%>%
  mutate(father_strata = as.numeric(father_strata))%>%
  ggplot(aes(scale(father_strata), scale(mean)))+
  geom_point()+geom_abline(intercept = 0, slope = 0.5)
```

##Bivariate Normal Distribution

When a pair or variables (x,y) appear in a scatterplot with oval shape, it can be modelled as a bivariate normal distriution. Two variables highly correlated have a thin shape, or a circle if no correlation exists.
both x and y must be approximately normally distributed.


```{r}
sd <- galton_heights %>%
  summarise(sd = sd(son))%>%
  pull(sd)
1-(1-0.5^2)*sd^2/sd^2 

```


##Basics of Evaluating Machine Learning Algorithms

```{r}
library(dslabs)

```
So there are 784 features to evaluate each letter

```{r}

```



##Comprehension Check: Confusion Matrix

```{r}
library(dslabs)
library(dplyr)
library(lubridate)

data("reported_heights")

dat <- mutate(reported_heights, date_time = ymd_hms(time_stamp)) %>%
  filter(date_time >= make_date(2016, 01, 25) & date_time < make_date(2016, 02, 1)) %>%
  mutate(type = ifelse(day(date_time) == 25 & hour(date_time) == 8 & between(minute(date_time), 15, 30), "inclass","online")) %>%
  select(sex, type)

y <- factor(dat$sex, c("Female", "Male"))
x <- dat$type
```

```{r}
dat %>%
  filter(type == "online")%>%
  group_by(sex)%>%
  summarise(n = n())%>%
  mutate(n = n/(sum(n)))
```

Accuracy of Guessing:


```{r}
dat %>%
  group_by(type)%>%
  summarise(n = n())%>%
  mutate(n = n/sum(n))

0.74*0.6216216	+ 0.26*0.6666667 
```

```{r}
y_hat <- ifelse(x == "inclass", "Female", "Male")
y_hat <- factor(y_hat, c("Female", "Male"))
confM <- table(predicted = y_hat, actual = y)
confM
#Sensitivity
confM[1,1]/(confM[1,1]+confM[2,1])
#Specificity
confM[2,2]/(confM[1,2]+confM[2,2])
#prevalence
mean("Female" == dat$sex)
```



##Comprehension Check: Practice with Machine Learning

```{r}
library(caret)
data(iris)
iris <- iris[-which(iris$Species=='setosa'),]
iris$Species <- factor(iris$Species, levels = c("virginica", "versicolor"))
y <- iris$Species

```
Create An even split train and test data sets:

```{r}
set.seed(2)
test_index <- createDataPartition(y,times=1,p=0.5,list=FALSE)
test <- iris[test_index,]
train <- iris[-test_index,]
```

Finding the Best Single Cutoff Predictor:
```{r}
train %>%
  group_by(Species)%>%
  summarise(m_sepal_length = mean(Sepal.Length), m_se_width = mean(Sepal.Width),
                                  m_pe_length = mean(Petal.Length), 
                                  m_pe_width = mean(Petal.Width))
```


```{r}
min(train$Petal.Length)
max(train$Petal.Length)
cutoff <- seq(3,6.9, by = 0.1)
accuracy <- map_dbl(cutoff, function(x){
  yhat <- ifelse(train$Petal.Length > x, "virginica", "versicolor")%>%
    factor(levels = levels(test$Species))
  F_meas(data = yhat, reference = factor(train$Species))
})
plot(cutoff,accuracy)
cutoff[which.max(accuracy)]

#Utilização da base de teste:
y_hat <- ifelse(test$Petal.Length > 4.7, "virginica", "versicolor")%>%
    factor(levels = levels(test$Species))
mean(y_hat == test$Species)

```


```{r}
min(train$Petal.Width)
max(train$Petal.Width)
cutoff1 <- seq(3,6.9, by = 0.1)
cutoff2 <- seq(1,2.5, by = 0.1)
accuracy2 <- map_dbl(cutoff2, function(x){
  yhat <- ifelse(train$Petal.Width > x, "virginica", "versicolor")%>%
    factor(levels = levels(test$Species))
  F_meas(data = yhat, reference = factor(train$Species))
})
plot(cutoff2,accuracy2)
cutoff2[which.max(accuracy2)]

#Utilização da base de teste:
y_hat <- ifelse(test$Petal.Length > 4.9 | test$Petal.Width > 1.7, "virginica", "versicolor")%>%
    factor(levels = levels(test$Species))
mean(y_hat == test$Species)

```

##Answer to last question because I've failed:

```{r}
library(caret)
data(iris)
iris <- iris[-which(iris$Species=='setosa'),]
y <- iris$Species

plot(iris,pch=21,bg=iris$Species)

set.seed(2)
test_index <- createDataPartition(y,times=1,p=0.5,list=FALSE)
test <- iris[test_index,]
train <- iris[-test_index,]

petalLengthRange <- seq(range(train[,3])[1],range(train[,3])[2],by=0.1)
petalWidthRange <- seq(range(train[,4])[1],range(train[,4])[2],by=0.1)
cutoffs <- expand.grid(petalLengthRange,petalWidthRange)

id <- sapply(seq(nrow(cutoffs)),function(i){
	y_hat <- ifelse(train[,3]>cutoffs[i,1] | train[,4]>cutoffs[i,2],'virginica','versicolor')
	mean(y_hat==train$Species)
	}) %>% which.max

optimalCutoff <- cutoffs[id,] %>% as.numeric
y_hat <- ifelse(test[,3]>optimalCutoff[1] & test[,4]>optimalCutoff[2],'virginica','versicolor')
mean(y_hat==test$Species)
```


;